{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGULajB+01MVJ1yANLlIIo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52222/GENERATIVE-AI/blob/main/Week1_2222_ASSIGNMENT_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADINA ISRAN\n",
        "\n",
        "2303A52222"
      ],
      "metadata": {
        "id": "ms-XIyqf0ubh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write Python code from scratch to find error metrics of deep learning model. Actual values and deep learning model predicted values are shown in Table 1. Also compare the results with the outcomes of libraries\n",
        "              YActual       YPred\n",
        "                20           20.5\n",
        "                30           30.3\n",
        "                40           40.2\n",
        "                50           50.6\n",
        "                60           60.7\n",
        "Tabela 1: YActual Vs. YPred"
      ],
      "metadata": {
        "id": "bgXH8owh036d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "y_actual = np.array([20, 30, 40, 50, 60])\n",
        "y_pred = np.array([20.5, 30.3, 40.2, 50.6, 60.7])\n",
        "\n",
        "# 1. Mean Absolute Error (MAE)\n",
        "mae = np.mean(np.abs(y_actual - y_pred))\n",
        "\n",
        "# 2. Mean Squared Error (MSE)\n",
        "mse = np.mean((y_actual - y_pred) ** 2)\n",
        "\n",
        "# 3. Root Mean Squared Error (RMSE)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# 4. R-squared (R2 Score)\n",
        "r2 = r2_score(y_actual, y_pred)\n",
        "\n",
        "# Output the results\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"R-squared (R2): {r2}\")\n",
        "\n",
        "# Comparing with sklearn metrics\n",
        "print(\"\\nComparison with scikit-learn metrics:\")\n",
        "print(f\"MAE (sklearn): {mean_absolute_error(y_actual, y_pred)}\")\n",
        "print(f\"MSE (sklearn): {mean_squared_error(y_actual, y_pred)}\")\n",
        "print(f\"RMSE (sklearn): {np.sqrt(mean_squared_error(y_actual, y_pred))}\")\n",
        "print(f\"R2 (sklearn): {r2_score(y_actual, y_pred)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BviYLGlz4Igp",
        "outputId": "64829754-6581-43f7-f438-746f372e103a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n",
            "R-squared (R2): 0.99877\n",
            "\n",
            "Comparison with scikit-learn metrics:\n",
            "MAE (sklearn): 0.4600000000000016\n",
            "MSE (sklearn): 0.24600000000000147\n",
            "RMSE (sklearn): 0.49598387070549127\n",
            "R2 (sklearn): 0.99877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write python code from scratch to find evaluation metrics of deep learning model.\n",
        "Actual values and deep learning model predicted values are shown in Table 2. Also compare the\n",
        "results with outcome of libraries\n",
        "          YActual    YP red\n",
        "          0 0 1       1 2 0\n",
        "          0 0 1       0 2 0\n",
        "          0 1 1       2 2 1\n",
        "          0 2 1       0 2 2\n",
        "          0 2 1       2 2 2\n",
        "Tabela 2: YActual Vs. YP red\n",
        "• Expected Leaning Outcomes from this assignment related to python\n",
        "– Students are able to understand deep learning model metrics\n",
        "– Students are able to write code in python to find deep learning model metrics\n",
        "– Students are able to use python libraries to find deep learning model metrics"
      ],
      "metadata": {
        "id": "RCF23oPL7FOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Actual and predicted values from Table 2 (binary classification)\n",
        "y_actual_class = np.array([0, 0, 1, 1, 2, 0, 0, 0, 1, 0, 2, 0, 0, 2, 1, 2, 2])\n",
        "y_pred_class = np.array([0, 0, 1, 0, 2, 0, 0, 1, 1, 2, 2, 1, 0, 2, 2, 2, 2])\n",
        "\n",
        "# 1. Accuracy\n",
        "accuracy = accuracy_score(y_actual_class, y_pred_class)\n",
        "\n",
        "# 2. Precision\n",
        "precision = precision_score(y_actual_class, y_pred_class, average='weighted', labels=np.unique(y_pred_class))\n",
        "\n",
        "# 3. Recall\n",
        "recall = recall_score(y_actual_class, y_pred_class, average='weighted', labels=np.unique(y_pred_class))\n",
        "\n",
        "# 4. F1 Score\n",
        "f1 = f1_score(y_actual_class, y_pred_class, average='weighted', labels=np.unique(y_pred_class))\n",
        "\n",
        "# Output the results\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "\n",
        "# Comparing with sklearn metrics\n",
        "print(\"\\nComparison with scikit-learn metrics:\")\n",
        "print(f\"Accuracy (sklearn): {accuracy_score(y_actual_class, y_pred_class)}\")\n",
        "print(f\"Precision (sklearn): {precision_score(y_actual_class, y_pred_class, average='weighted')}\")\n",
        "print(f\"Recall (sklearn): {recall_score(y_actual_class, y_pred_class, average='weighted')}\")\n",
        "print(f\"F1 Score (sklearn): {f1_score(y_actual_class, y_pred_class, average='weighted')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjW0thPu7dpb",
        "outputId": "98d4a06f-1273-445c-deac-2d6031bdca05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7058823529411765\n",
            "Precision: 0.7198879551820729\n",
            "Recall: 0.7058823529411765\n",
            "F1 Score: 0.6988795518207284\n",
            "\n",
            "Comparison with scikit-learn metrics:\n",
            "Accuracy (sklearn): 0.7058823529411765\n",
            "Precision (sklearn): 0.7198879551820729\n",
            "Recall (sklearn): 0.7058823529411765\n",
            "F1 Score (sklearn): 0.6988795518207284\n"
          ]
        }
      ]
    }
  ]
}